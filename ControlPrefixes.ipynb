{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b666c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634a5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db19b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "pretrained=GPT2LMHeadModel.from_pretrained('gpt2-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f1313e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 1280)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "pretrained.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86c9ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token: <|endoftext|>\n",
      "eos_token: <|endoftext|>\n",
      "pad_token_id: 50257\n"
     ]
    }
   ],
   "source": [
    "print(\"bos_token:\", tokenizer.bos_token)\n",
    "print(\"eos_token:\", tokenizer.eos_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844382bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/webnlg/train.json', 'r') as f:\n",
    "    dict_train=json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8cff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18025 Categories\n",
      "18025 Triples\n",
      "18025 Texts\n"
     ]
    }
   ],
   "source": [
    "data_category=[]\n",
    "data_triple=[]\n",
    "data_text=[]\n",
    "\n",
    "for index, data in enumerate(dict_train['entries']):\n",
    "    triples=data[str(index+1)]['modifiedtripleset']\n",
    "    triple_proc=\"\"\n",
    "    for triple in triples:\n",
    "        subj, prop, obj=triple['subject'], triple['property'], triple['object']\n",
    "        triple_proc+=\"| {} : {} : {} \".format(subj, prop, obj)\n",
    "        \n",
    "    texts=data[str(index+1)]['lexicalisations']\n",
    "    for text in texts:\n",
    "        if text['comment']!=\"good\": continue\n",
    "            \n",
    "        data_category.append(data[str(index+1)]['category'])\n",
    "        data_triple.append(triple_proc)\n",
    "        data_text.append(text['lex'])\n",
    "        \n",
    "print(len(data_category), 'Categories')\n",
    "print(len(data_triple), \"Triples\")\n",
    "print(len(data_text), \"Texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22fb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Prefix Length\n",
    "gen_seqlen=10\n",
    "\n",
    "# Hyperparams\n",
    "batch_size=5\n",
    "accumulation_steps=1\n",
    "epochs=10\n",
    "lr=7e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3388facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D2TDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_category, data_triple, data_text):\n",
    "        \n",
    "        self.category=data_category\n",
    "        self.data=[]\n",
    "        self.label=[]\n",
    "        \n",
    "        for index, triple in enumerate(data_triple):\n",
    "            data=tokenizer.encode(triple+tokenizer.bos_token+data_text[index]+tokenizer.eos_token)\n",
    "            self.data.append(data)\n",
    "            \n",
    "            label=tokenizer.encode(triple+tokenizer.bos_token+data_text[index]+tokenizer.eos_token)\n",
    "            sep=label.index(tokenizer.bos_token_id)+1\n",
    "            label[:sep]=[-100]*sep\n",
    "            self.label.append(label)\n",
    "            \n",
    "        print(len(self.data), \"Data\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.category[idx], self.data[idx], self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c5b5923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_len=0\n",
    "    for _, data, _ in batch:\n",
    "        if len(data)>max_len: max_len=len(data)\n",
    "            \n",
    "    categories=[]\n",
    "    datas=[]\n",
    "    labels=[]\n",
    "    for category, data, label in batch:\n",
    "        categories.append(category)\n",
    "        \n",
    "        data.extend([tokenizer.pad_token_id]*(max_len-len(data)))\n",
    "        datas.append(data)\n",
    "        \n",
    "        label.extend([tokenizer.pad_token_id]*(max_len-len(label)))\n",
    "        labels.append(label)\n",
    "        \n",
    "    return categories, torch.tensor(datas), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3031907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18025 Data\n"
     ]
    }
   ],
   "source": [
    "dataset=D2TDataset(tokenizer=tokenizer, data_category=data_category, data_triple=data_triple, data_text=data_text)\n",
    "dataloader=DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45cb52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlPrefixes(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained, controls, gen_seqlen=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        self.config=self.pretrained.config\n",
    "        for param in self.pretrained.parameters():\n",
    "            param.requires_grad=False\n",
    "            \n",
    "        # Control Prefixes: Attributes\n",
    "        self.controls=controls\n",
    "        print(self.controls)\n",
    "        \n",
    "        # General Prefix Length\n",
    "        self.gen_seqlen=gen_seqlen\n",
    "        \n",
    "        self.input_tokens=torch.arange(gen_seqlen).long()\n",
    "        self.wte=nn.Embedding(len(controls)+gen_seqlen, self.config.n_embd)\n",
    "        self.control_trans=nn.Sequential(\n",
    "            nn.Linear(self.config.n_embd, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, self.config.n_layer*2*self.config.n_embd)\n",
    "        )\n",
    "        self.dropout=nn.Dropout(p=0.0)\n",
    "        \n",
    "        self.get_prompt=self.get_prompt_fn\n",
    "        \n",
    "    def get_prompt_fn(self, bsz, controls):\n",
    "        # Control Prefixes\n",
    "        controls=[self.gen_seqlen+self.controls.index(c) for c in controls]\n",
    "        controls=torch.tensor(controls).unsqueeze(1)\n",
    "        # General Prefix\n",
    "        input_tokens=self.input_tokens.unsqueeze(0).expand(bsz, -1)\n",
    "        # [Control Prefixes, General Prefix]\n",
    "        input_tokens=torch.cat((controls, input_tokens), dim=1).to(device)\n",
    "        #print(input_tokens)\n",
    "        \n",
    "        temp_control=self.wte(input_tokens)\n",
    "        past_key_values=self.control_trans(temp_control)\n",
    "        bsz, seqlen, _=past_key_values.shape\n",
    "        past_key_values=past_key_values.view(bsz, seqlen, 2*self.config.n_layer, self.config.n_head, int(self.config.n_embd/self.config.n_head))\n",
    "        past_key_values=self.dropout(past_key_values)\n",
    "        past_key_values=past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "        \n",
    "        return past_key_values\n",
    "        \n",
    "    def forward(self, controls, input_ids, labels):        \n",
    "        bsz=input_ids.shape[0]\n",
    "        past_key_values_prompt=self.get_prompt(bsz=bsz, controls=controls)\n",
    "        outputs=self.pretrained(input_ids=input_ids, labels=labels, past_key_values=past_key_values_prompt)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c36499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ComicsCharacter', 'WrittenWork', 'Building', 'Monument', 'SportsTeam', 'Astronaut', 'Food', 'City', 'Airport', 'University']\n",
      "epoch 1 step 300.0 loss 0.8324\n",
      "epoch 1 step 600.0 loss 0.5727\n",
      "epoch 1 step 900.0 loss 0.2231\n",
      "epoch 1 step 1200.0 loss 0.5914\n",
      "epoch 1 step 1500.0 loss 0.2977\n",
      "epoch 1 step 1800.0 loss 0.2371\n",
      "epoch 1 step 2100.0 loss 0.4438\n",
      "epoch 1 step 2400.0 loss 0.4288\n",
      "epoch 1 step 2700.0 loss 0.2354\n",
      "epoch 1 step 3000.0 loss 0.2000\n",
      "epoch 1 step 3300.0 loss 0.4080\n",
      "epoch 1 step 3600.0 loss 0.2607\n",
      "epoch 2 step 300.0 loss 0.1324\n",
      "epoch 2 step 600.0 loss 0.3924\n",
      "epoch 2 step 900.0 loss 0.1692\n",
      "epoch 2 step 1200.0 loss 0.2124\n",
      "epoch 2 step 1500.0 loss 0.2115\n",
      "epoch 2 step 1800.0 loss 0.2914\n",
      "epoch 2 step 2100.0 loss 0.3101\n",
      "epoch 2 step 2400.0 loss 0.2478\n",
      "epoch 2 step 2700.0 loss 0.2288\n",
      "epoch 2 step 3000.0 loss 0.3006\n",
      "epoch 2 step 3300.0 loss 0.1820\n",
      "epoch 2 step 3600.0 loss 0.3325\n",
      "epoch 3 step 300.0 loss 0.1292\n",
      "epoch 3 step 600.0 loss 0.1104\n",
      "epoch 3 step 900.0 loss 0.1022\n",
      "epoch 3 step 1200.0 loss 0.0938\n",
      "epoch 3 step 1500.0 loss 0.0971\n",
      "epoch 3 step 1800.0 loss 0.2199\n",
      "epoch 3 step 2100.0 loss 0.1011\n",
      "epoch 3 step 2400.0 loss 0.0888\n",
      "epoch 3 step 2700.0 loss 0.1129\n",
      "epoch 3 step 3000.0 loss 0.2346\n",
      "epoch 3 step 3300.0 loss 0.1195\n",
      "epoch 3 step 3600.0 loss 0.2368\n",
      "epoch 4 step 300.0 loss 0.1037\n",
      "epoch 4 step 600.0 loss 0.0467\n",
      "epoch 4 step 900.0 loss 0.0375\n",
      "epoch 4 step 1200.0 loss 0.1892\n",
      "epoch 4 step 1500.0 loss 0.0549\n",
      "epoch 4 step 1800.0 loss 0.1135\n",
      "epoch 4 step 2100.0 loss 0.1581\n",
      "epoch 4 step 2400.0 loss 0.0525\n",
      "epoch 4 step 2700.0 loss 0.1163\n",
      "epoch 4 step 3000.0 loss 0.1607\n",
      "epoch 4 step 3300.0 loss 0.1217\n",
      "epoch 4 step 3600.0 loss 0.1573\n",
      "epoch 5 step 300.0 loss 0.0597\n",
      "epoch 5 step 600.0 loss 0.1364\n",
      "epoch 5 step 900.0 loss 0.0730\n",
      "epoch 5 step 1200.0 loss 0.1375\n",
      "epoch 5 step 1500.0 loss 0.1147\n",
      "epoch 5 step 1800.0 loss 0.1172\n",
      "epoch 5 step 2100.0 loss 0.0868\n",
      "epoch 5 step 2400.0 loss 0.0758\n",
      "epoch 5 step 2700.0 loss 0.1108\n",
      "epoch 5 step 3000.0 loss 0.0771\n",
      "epoch 5 step 3300.0 loss 0.0463\n",
      "epoch 5 step 3600.0 loss 0.0581\n",
      "epoch 6 step 300.0 loss 0.0841\n",
      "epoch 6 step 600.0 loss 0.1109\n",
      "epoch 6 step 900.0 loss 0.0992\n",
      "epoch 6 step 1200.0 loss 0.1219\n",
      "epoch 6 step 1500.0 loss 0.0749\n",
      "epoch 6 step 1800.0 loss 0.1766\n",
      "epoch 6 step 2100.0 loss 0.1152\n",
      "epoch 6 step 2400.0 loss 0.0744\n",
      "epoch 6 step 2700.0 loss 0.1160\n",
      "epoch 6 step 3000.0 loss 0.0764\n",
      "epoch 6 step 3300.0 loss 0.0708\n",
      "epoch 6 step 3600.0 loss 0.1233\n",
      "epoch 7 step 300.0 loss 0.0482\n",
      "epoch 7 step 600.0 loss 0.1029\n",
      "epoch 7 step 900.0 loss 0.0540\n",
      "epoch 7 step 1200.0 loss 0.0564\n",
      "epoch 7 step 1500.0 loss 0.0887\n",
      "epoch 7 step 1800.0 loss 0.1174\n",
      "epoch 7 step 2100.0 loss 0.0744\n",
      "epoch 7 step 2400.0 loss 0.0777\n",
      "epoch 7 step 2700.0 loss 0.0941\n",
      "epoch 7 step 3000.0 loss 0.0807\n",
      "epoch 7 step 3300.0 loss 0.0741\n",
      "epoch 7 step 3600.0 loss 0.0821\n",
      "epoch 8 step 300.0 loss 0.1077\n",
      "epoch 8 step 600.0 loss 0.0781\n",
      "epoch 8 step 900.0 loss 0.0892\n",
      "epoch 8 step 1200.0 loss 0.0404\n",
      "epoch 8 step 1500.0 loss 0.1251\n",
      "epoch 8 step 1800.0 loss 0.0682\n",
      "epoch 8 step 2100.0 loss 0.1227\n",
      "epoch 8 step 2400.0 loss 0.0622\n",
      "epoch 8 step 2700.0 loss 0.0697\n",
      "epoch 8 step 3000.0 loss 0.0852\n",
      "epoch 8 step 3300.0 loss 0.0305\n",
      "epoch 8 step 3600.0 loss 0.1234\n",
      "epoch 9 step 300.0 loss 0.0722\n",
      "epoch 9 step 600.0 loss 0.1464\n",
      "epoch 9 step 900.0 loss 0.0221\n",
      "epoch 9 step 1200.0 loss 0.1121\n",
      "epoch 9 step 1500.0 loss 0.0799\n",
      "epoch 9 step 1800.0 loss 0.0873\n",
      "epoch 9 step 2100.0 loss 0.1192\n",
      "epoch 9 step 2400.0 loss 0.0832\n",
      "epoch 9 step 2700.0 loss 0.0760\n",
      "epoch 9 step 3000.0 loss 0.0809\n",
      "epoch 9 step 3300.0 loss 0.0792\n",
      "epoch 9 step 3600.0 loss 0.0544\n",
      "epoch 10 step 300.0 loss 0.0512\n",
      "epoch 10 step 600.0 loss 0.0310\n",
      "epoch 10 step 900.0 loss 0.0980\n",
      "epoch 10 step 1200.0 loss 0.0659\n",
      "epoch 10 step 1500.0 loss 0.1359\n",
      "epoch 10 step 1800.0 loss 0.0742\n",
      "epoch 10 step 2100.0 loss 0.0776\n",
      "epoch 10 step 2400.0 loss 0.1282\n",
      "epoch 10 step 2700.0 loss 0.0735\n",
      "epoch 10 step 3000.0 loss 0.0383\n",
      "epoch 10 step 3300.0 loss 0.1053\n",
      "epoch 10 step 3600.0 loss 0.1009\n"
     ]
    }
   ],
   "source": [
    "model=ControlPrefixes(pretrained=pretrained, controls=list(set(data_category)), gen_seqlen=gen_seqlen)\n",
    "\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler=get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(0.03*epochs*len(dataset)/batch_size),\n",
    "    num_training_steps=int(epochs*len(dataset)/(accumulation_steps*batch_size))\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_total=0\n",
    "    optimizer.zero_grad()\n",
    "    for step, (category, data, label) in enumerate(dataloader):\n",
    "        data=data.to(device)\n",
    "        label=label.to(device)\n",
    "        \n",
    "        outputs=model(controls=category, input_ids=data, labels=label)\n",
    "        \n",
    "        loss=outputs[0]/accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_total+=loss.item()\n",
    "        \n",
    "        if (step+1)%accumulation_steps==0:\n",
    "            if (step+1)%(300*accumulation_steps)==0:\n",
    "                print(f'epoch {epoch+1} step {(step+1)/accumulation_steps} loss {loss_total:.4f}')\n",
    "            loss_total=0\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "model.eval()\n",
    "model.to(torch.device('cpu'))\n",
    "\n",
    "torch.save(model, './model/'+f'control-prefixes-tuned_Large_preseqlen{gen_seqlen}_batch{int(accumulation_steps*batch_size)}_epoch{epochs}_lr{lr}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98deeafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
