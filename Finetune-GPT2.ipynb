{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e31bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0981f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca3d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model=GPT2LMHeadModel.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e08ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 1024)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11afb15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token: <|endoftext|>\n",
      "eos_token: <|endoftext|>\n",
      "pad_token_id: 50257\n"
     ]
    }
   ],
   "source": [
    "print(\"bos_token:\", tokenizer.bos_token)\n",
    "print(\"eos_token:\", tokenizer.eos_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f3cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/webnlg/train.json', 'r') as f:\n",
    "    dict_train=json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc8ff540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18025 Triples\n",
      "18025 Texts\n"
     ]
    }
   ],
   "source": [
    "data_triple=[]\n",
    "data_text=[]\n",
    "\n",
    "for index, data in enumerate(dict_train['entries']):\n",
    "    triples=data[str(index+1)]['modifiedtripleset']\n",
    "    triple_proc=\"\"\n",
    "    for triple in triples:\n",
    "        subj, prop, obj=triple['subject'], triple['property'], triple['object']\n",
    "        triple_proc+=\"| {} : {} : {} \".format(subj, prop, obj)\n",
    "        \n",
    "    texts=data[str(index+1)]['lexicalisations']\n",
    "    for text in texts:\n",
    "        if text['comment']!=\"good\": continue\n",
    "            \n",
    "        data_triple.append(triple_proc)\n",
    "        data_text.append(text['lex'])\n",
    "        \n",
    "print(len(data_triple), \"Triples\")\n",
    "print(len(data_text), \"Texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60e76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "accumulation_steps=6\n",
    "epochs=10\n",
    "lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bbec8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D2TDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_triple, data_text):\n",
    "        self.data=[]\n",
    "        self.label=[]\n",
    "        \n",
    "        for index, triple in enumerate(data_triple):\n",
    "            data=tokenizer.encode(triple+tokenizer.bos_token+data_text[index]+tokenizer.eos_token)\n",
    "            self.data.append(data)\n",
    "            \n",
    "            label=tokenizer.encode(triple+tokenizer.bos_token+data_text[index]+tokenizer.eos_token)\n",
    "            sep=label.index(tokenizer.bos_token_id)+1\n",
    "            label[:sep]=[-100]*sep\n",
    "            self.label.append(label)\n",
    "            \n",
    "        print(len(self.data), \"Data\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaac3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_len=0\n",
    "    for data, _ in batch:\n",
    "        if len(data)>max_len: max_len=len(data)\n",
    "            \n",
    "    datas=[]\n",
    "    labels=[]\n",
    "    for data, label in batch:\n",
    "        data.extend([tokenizer.pad_token_id]*(max_len-len(data)))\n",
    "        datas.append(data)\n",
    "        \n",
    "        label.extend([tokenizer.pad_token_id]*(max_len-len(label)))\n",
    "        labels.append(label)\n",
    "        \n",
    "    return torch.tensor(datas), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f2c236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18025 Data\n"
     ]
    }
   ],
   "source": [
    "dataset=D2TDataset(tokenizer=tokenizer, data_triple=data_triple, data_text=data_text)\n",
    "dataloader=DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dc008e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 step 300.0 loss 3.1598\n",
      "epoch 1 step 600.0 loss 1.8543\n",
      "epoch 1 step 900.0 loss 1.5883\n",
      "epoch 1 step 1200.0 loss 1.5418\n",
      "epoch 1 step 1500.0 loss 1.7015\n",
      "epoch 1 step 1800.0 loss 1.1925\n",
      "epoch 1 step 2100.0 loss 1.1479\n",
      "epoch 1 step 2400.0 loss 1.1715\n",
      "epoch 1 step 2700.0 loss 1.0070\n",
      "epoch 1 step 3000.0 loss 0.8128\n",
      "epoch 2 step 300.0 loss 0.7740\n",
      "epoch 2 step 600.0 loss 1.0153\n",
      "epoch 2 step 900.0 loss 0.8562\n",
      "epoch 2 step 1200.0 loss 0.9356\n",
      "epoch 2 step 1500.0 loss 0.9994\n",
      "epoch 2 step 1800.0 loss 0.5988\n",
      "epoch 2 step 2100.0 loss 0.7066\n",
      "epoch 2 step 2400.0 loss 0.8386\n",
      "epoch 2 step 2700.0 loss 0.8179\n",
      "epoch 2 step 3000.0 loss 0.7492\n",
      "epoch 3 step 300.0 loss 0.5637\n",
      "epoch 3 step 600.0 loss 1.1142\n",
      "epoch 3 step 900.0 loss 0.8546\n",
      "epoch 3 step 1200.0 loss 0.9596\n",
      "epoch 3 step 1500.0 loss 0.6909\n",
      "epoch 3 step 1800.0 loss 0.4799\n",
      "epoch 3 step 2100.0 loss 0.7348\n",
      "epoch 3 step 2400.0 loss 0.9001\n",
      "epoch 3 step 2700.0 loss 0.8188\n",
      "epoch 3 step 3000.0 loss 0.7017\n",
      "epoch 4 step 300.0 loss 0.7981\n",
      "epoch 4 step 600.0 loss 1.0078\n",
      "epoch 4 step 900.0 loss 0.8361\n",
      "epoch 4 step 1200.0 loss 0.7232\n",
      "epoch 4 step 1500.0 loss 0.4810\n",
      "epoch 4 step 1800.0 loss 0.6428\n",
      "epoch 4 step 2100.0 loss 0.6496\n",
      "epoch 4 step 2400.0 loss 0.6988\n",
      "epoch 4 step 2700.0 loss 0.6694\n",
      "epoch 4 step 3000.0 loss 0.6040\n",
      "epoch 5 step 300.0 loss 0.6381\n",
      "epoch 5 step 600.0 loss 0.7303\n",
      "epoch 5 step 900.0 loss 0.7453\n",
      "epoch 5 step 1200.0 loss 0.7840\n",
      "epoch 5 step 1500.0 loss 0.7963\n",
      "epoch 5 step 1800.0 loss 0.7654\n",
      "epoch 5 step 2100.0 loss 0.5978\n",
      "epoch 5 step 2400.0 loss 0.5357\n",
      "epoch 5 step 2700.0 loss 0.5003\n",
      "epoch 5 step 3000.0 loss 0.7501\n",
      "epoch 6 step 300.0 loss 0.5491\n",
      "epoch 6 step 600.0 loss 0.5450\n",
      "epoch 6 step 900.0 loss 0.4986\n",
      "epoch 6 step 1200.0 loss 0.4671\n",
      "epoch 6 step 1500.0 loss 0.7094\n",
      "epoch 6 step 1800.0 loss 0.4246\n",
      "epoch 6 step 2100.0 loss 0.4731\n",
      "epoch 6 step 2400.0 loss 1.0841\n",
      "epoch 6 step 2700.0 loss 0.6666\n",
      "epoch 6 step 3000.0 loss 0.4171\n",
      "epoch 7 step 300.0 loss 0.4650\n",
      "epoch 7 step 600.0 loss 0.4044\n",
      "epoch 7 step 900.0 loss 0.4350\n",
      "epoch 7 step 1200.0 loss 0.5517\n",
      "epoch 7 step 1500.0 loss 0.5902\n",
      "epoch 7 step 1800.0 loss 0.4882\n",
      "epoch 7 step 2100.0 loss 0.3090\n",
      "epoch 7 step 2400.0 loss 0.6287\n",
      "epoch 7 step 2700.0 loss 0.7019\n",
      "epoch 7 step 3000.0 loss 0.5101\n",
      "epoch 8 step 300.0 loss 0.6559\n",
      "epoch 8 step 600.0 loss 0.4819\n",
      "epoch 8 step 900.0 loss 0.7320\n",
      "epoch 8 step 1200.0 loss 0.5939\n",
      "epoch 8 step 1500.0 loss 0.7274\n",
      "epoch 8 step 1800.0 loss 0.5135\n",
      "epoch 8 step 2100.0 loss 0.3238\n",
      "epoch 8 step 2400.0 loss 0.5179\n",
      "epoch 8 step 2700.0 loss 0.6089\n",
      "epoch 8 step 3000.0 loss 0.5523\n",
      "epoch 9 step 300.0 loss 0.7318\n",
      "epoch 9 step 600.0 loss 0.6312\n",
      "epoch 9 step 900.0 loss 0.6499\n",
      "epoch 9 step 1200.0 loss 0.5395\n",
      "epoch 9 step 1500.0 loss 0.6354\n",
      "epoch 9 step 1800.0 loss 0.4334\n",
      "epoch 9 step 2100.0 loss 0.5776\n",
      "epoch 9 step 2400.0 loss 0.3347\n",
      "epoch 9 step 3000.0 loss 0.6348\n",
      "epoch 10 step 300.0 loss 0.3737\n",
      "epoch 10 step 600.0 loss 0.3433\n",
      "epoch 10 step 900.0 loss 0.3597\n",
      "epoch 10 step 1200.0 loss 0.4226\n",
      "epoch 10 step 1500.0 loss 0.5760\n",
      "epoch 10 step 1800.0 loss 0.4149\n",
      "epoch 10 step 2100.0 loss 0.4455\n",
      "epoch 10 step 2400.0 loss 0.5399\n",
      "epoch 10 step 2700.0 loss 0.6260\n",
      "epoch 10 step 3000.0 loss 0.6234\n"
     ]
    }
   ],
   "source": [
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler=get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000,\n",
    "    num_training_steps=int(epochs*len(dataset)/(accumulation_steps*batch_size))\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_total=0\n",
    "    optimizer.zero_grad()\n",
    "    for step, (data, label) in enumerate(dataloader):\n",
    "        data=data.to(device)\n",
    "        label=label.to(device)\n",
    "        \n",
    "        outputs=model(data, labels=label)\n",
    "        \n",
    "        loss=outputs[0]/accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_total+=loss.item()\n",
    "        \n",
    "        if (step+1)%accumulation_steps==0:\n",
    "            if (step+1)%(300*accumulation_steps)==0:\n",
    "                print(f'epoch {epoch+1} step {(step+1)/accumulation_steps} loss {loss_total:.4f}')\n",
    "            loss_total=0\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "model.eval()\n",
    "model.to(torch.device('cpu'))\n",
    "\n",
    "torch.save(model, './model/'+f'finetuned_batch{int(accumulation_steps*batch_size)}_epoch{epochs}_lr{lr}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa5409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
