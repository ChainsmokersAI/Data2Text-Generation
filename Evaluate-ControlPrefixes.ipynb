{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3411d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d5a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "device=torch.device('cuda:2')\n",
    "\n",
    "model_path='./model/control-prefixes-tuned_preseqlen5_batch5_epoch10_lr5e-05.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823f8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlPrefixes(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained, controls, gen_seqlen=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        self.config=self.pretrained.config\n",
    "        for param in self.pretrained.parameters():\n",
    "            param.requires_grad=False\n",
    "            \n",
    "        # Control Prefixes: Attributes\n",
    "        self.controls=controls\n",
    "        print(self.controls)\n",
    "        \n",
    "        # General Prefix Length\n",
    "        self.gen_seqlen=gen_seqlen\n",
    "        \n",
    "        self.input_tokens=torch.arange(gen_seqlen).long()\n",
    "        self.wte=nn.Embedding(len(controls)+gen_seqlen, self.config.n_embd)\n",
    "        self.control_trans=nn.Sequential(\n",
    "            nn.Linear(self.config.n_embd, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, self.config.n_layer*2*self.config.n_embd)\n",
    "        )\n",
    "        self.dropout=nn.Dropout(p=0.0)\n",
    "        \n",
    "        self.get_prompt=self.get_prompt_fn\n",
    "        \n",
    "    def get_prompt_fn(self, bsz, controls):\n",
    "        # General Prefix\n",
    "        input_tokens=self.input_tokens.unsqueeze(0).expand(bsz, -1)\n",
    "        # [Control Prefixes, General Prefix]\n",
    "        input_tokens=torch.cat((controls, input_tokens), dim=1).to(device)\n",
    "        #print(input_tokens)\n",
    "        \n",
    "        temp_control=self.wte(input_tokens)\n",
    "        past_key_values=self.control_trans(temp_control)\n",
    "        bsz, seqlen, _=past_key_values.shape\n",
    "        past_key_values=past_key_values.view(bsz, seqlen, 2*self.config.n_layer, self.config.n_head, int(self.config.n_embd/self.config.n_head))\n",
    "        past_key_values=self.dropout(past_key_values)\n",
    "        past_key_values=past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "        \n",
    "        return past_key_values\n",
    "        \n",
    "    def forward(self, controls, input_ids, labels):\n",
    "        # Control Prefixes\n",
    "        controls=[self.gen_seqlen+self.controls.index(c) for c in controls]\n",
    "        controls=torch.tensor(controls).unsqueeze(1)\n",
    "        \n",
    "        bsz=input_ids.shape[0]\n",
    "        past_key_values_prompt=self.get_prompt(bsz=bsz, controls=controls)\n",
    "        outputs=self.pretrained(input_ids=input_ids, labels=labels, past_key_values=past_key_values_prompt)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8253ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model=torch.load(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db565c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token_id: 50257\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f567a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a26a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/webnlg/train.json', 'r') as f:\n",
    "    dict_train=json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9ce04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Categories in Train Set\n",
      "['Astronaut', 'University', 'ComicsCharacter', 'Airport', 'Food', 'Building', 'City', 'WrittenWork', 'Monument', 'SportsTeam']\n"
     ]
    }
   ],
   "source": [
    "categories_seen=[]\n",
    "\n",
    "for index, data in enumerate(dict_train['entries']):\n",
    "    categories_seen.append(data[str(index+1)]['category'])\n",
    "    \n",
    "categories_seen=list(set(categories_seen))\n",
    "print(len(categories_seen), \"Categories in Train Set\")\n",
    "print(categories_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c62f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/webnlg/test.json', 'r') as f:\n",
    "    dict_test=json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f9c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_unseen=[]\n",
    "\n",
    "controls_seen=[]\n",
    "triples_seen=[]\n",
    "refs_seen=[]\n",
    "\n",
    "controls_unseen=[]\n",
    "triples_unseen=[]\n",
    "refs_unseen=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "586157ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, data in enumerate(dict_test['entries']):\n",
    "    data=data[str(index+1)]\n",
    "    \n",
    "    triple_proc=\"\"\n",
    "    for triple in data['modifiedtripleset']:\n",
    "        subj, prop, obj=triple['subject'], triple['property'], triple['object']\n",
    "        triple_proc+=\"| {} : {} : {} \".format(subj, prop, obj)\n",
    "        \n",
    "    texts=data['lexicalisations']\n",
    "    \n",
    "    if data['category'] not in categories_seen:\n",
    "        categories_unseen.append(data['category'])\n",
    "        \n",
    "        controls_unseen.append(data['category'])\n",
    "        triples_unseen.append(triple_proc)\n",
    "        refs_unseen.append([text['lex'] for text in texts])\n",
    "        continue\n",
    "        \n",
    "    controls_seen.append(data['category'])\n",
    "    triples_seen.append(triple_proc)\n",
    "    refs_seen.append([text['lex'] for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b38575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Unseen Categories\n",
      "['CelestialBody', 'Artist', 'MeanOfTransportation', 'Politician', 'Athlete']\n",
      "=====\n",
      "971 Seen Data\n",
      "891 Unseen Data\n"
     ]
    }
   ],
   "source": [
    "categories_unseen=list(set(categories_unseen))\n",
    "print(len(categories_unseen), \"Unseen Categories\")\n",
    "print(categories_unseen)\n",
    "print(\"=====\")\n",
    "\n",
    "print(len(triples_seen), \"Seen Data\")\n",
    "print(len(triples_unseen), \"Unseen Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7a317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "754592f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen Categories\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "BLEU Score: 50.45\n"
     ]
    }
   ],
   "source": [
    "scores_seen=[]\n",
    "generations=\"\"\n",
    "\n",
    "print(\"Seen Categories\")\n",
    "\n",
    "for index, triple in enumerate(triples_seen):\n",
    "    if (index+1)%100==0: print(index+1)\n",
    "        \n",
    "    input_=triple+tokenizer.bos_token\n",
    "    len_=len(input_)\n",
    "    for i in range(100):\n",
    "        input_ids=tokenizer.encode(input_)\n",
    "        output=model(controls=[controls_seen[index]], input_ids=torch.tensor([input_ids]).to(device), labels=None)\n",
    "        pred=tokenizer.decode(torch.argmax(output.logits[0][-1]))\n",
    "        \n",
    "        if pred==tokenizer.eos_token: break\n",
    "            \n",
    "        input_+=pred\n",
    "    cand=input_[len_:]\n",
    "    generations+=cand+\"\\n\"\n",
    "    \n",
    "    bleu_score=sentence_bleu(\n",
    "        [ref.split() for ref in refs_seen[index]],\n",
    "        cand.split(),\n",
    "        smoothing_function=SmoothingFunction().method4\n",
    "    )\n",
    "    scores_seen.append(bleu_score)\n",
    "print(\"BLEU Score: {:.2f}\".format(100*sum(scores_seen)/len(scores_seen)))\n",
    "\n",
    "with open('./generation/'+model_path.split(\"/\")[-1][:-3]+\"_Seen\", 'w') as f:\n",
    "    f.write(generations)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c108724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen Categories\n",
      "BLEU Score: 50.45\n"
     ]
    }
   ],
   "source": [
    "scores_seen=[]\n",
    "\n",
    "with open('./generation/'+model_path.split(\"/\")[-1][:-3]+\"_Seen\", 'r') as f:\n",
    "    cands_seen=f.read().split(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "print(\"Seen Categories\")\n",
    "\n",
    "for index, refs in enumerate(refs_seen):\n",
    "    bleu_score=sentence_bleu(\n",
    "        [ref.split() for ref in refs],\n",
    "        cands_seen[index].split(),\n",
    "        smoothing_function=SmoothingFunction().method4\n",
    "    )\n",
    "    scores_seen.append(bleu_score)\n",
    "print(\"BLEU Score: {:.2f}\".format(100*sum(scores_seen)/len(scores_seen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91a668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93367a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls_mapping={\n",
    "    'Artist': 'WrittenWork',\n",
    "    'Athlete': 'SportsTeam',\n",
    "    'CelestialBody': 'Astronaut',\n",
    "    'MeanOfTransportation': 'Airport',\n",
    "    'Politician': 'Monument'\n",
    "}\n",
    "controls_unseen=[controls_mapping[c] for c in controls_unseen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "186aea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen Categories\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "BLEU Score: 28.64\n"
     ]
    }
   ],
   "source": [
    "scores_unseen=[]\n",
    "generations=\"\"\n",
    "\n",
    "print(\"Unseen Categories\")\n",
    "\n",
    "for index, triple in enumerate(triples_unseen):\n",
    "    if (index+1)%100==0: print(index+1)\n",
    "        \n",
    "    input_=triple+tokenizer.bos_token\n",
    "    len_=len(input_)\n",
    "    for i in range(100):\n",
    "        input_ids=tokenizer.encode(input_)\n",
    "        output=model(controls=[controls_unseen[index]], input_ids=torch.tensor([input_ids]).to(device), labels=None)\n",
    "        pred=tokenizer.decode(torch.argmax(output.logits[0][-1]))\n",
    "        \n",
    "        if pred==tokenizer.eos_token: break\n",
    "            \n",
    "        input_+=pred\n",
    "    cand=input_[len_:]\n",
    "    generations+=cand+\"\\n\"\n",
    "    \n",
    "    bleu_score=sentence_bleu(\n",
    "        [ref.split() for ref in refs_unseen[index]],\n",
    "        cand.split(),\n",
    "        smoothing_function=SmoothingFunction().method4\n",
    "    )\n",
    "    scores_unseen.append(bleu_score)\n",
    "print(\"BLEU Score: {:.2f}\".format(100*sum(scores_unseen)/len(scores_unseen)))\n",
    "\n",
    "with open('./generation/'+model_path.split(\"/\")[-1][:-3]+\"_Unseen\", 'w') as f:\n",
    "    f.write(generations)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c14830c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen Categories\n",
      "BLEU Score: 28.64\n"
     ]
    }
   ],
   "source": [
    "scores_unseen=[]\n",
    "\n",
    "with open('./generation/'+model_path.split(\"/\")[-1][:-3]+\"_Unseen\", 'r') as f:\n",
    "    cands_unseen=f.read().split(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "print(\"Unseen Categories\")\n",
    "\n",
    "for index, refs in enumerate(refs_unseen):\n",
    "    bleu_score=sentence_bleu(\n",
    "        [ref.split() for ref in refs],\n",
    "        cands_unseen[index].split(),\n",
    "        smoothing_function=SmoothingFunction().method4\n",
    "    )\n",
    "    scores_unseen.append(bleu_score)\n",
    "print(\"BLEU Score: {:.2f}\".format(100*sum(scores_unseen)/len(scores_unseen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b067b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
